{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vigrahalabhanu3/FFML-226Q1A4409/blob/main/Module_01_Lab_01_Features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6I1yI79fbLD"
      },
      "source": [
        "# Extracting features from data\n",
        "\n",
        "FMML Module 1, Lab 1<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OimBnfcpvcNS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0195118-e1bf-4c45-e0e1-93d4486ab8fd"
      },
      "source": [
        "! pip install wikipedia\n",
        "\n",
        "import wikipedia\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import re\n",
        "import unicodedata\n",
        "import plotly.express as px\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.11.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.7.22)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=7cef8b53b083ece188454d39a995d96f329670f6a13cf2cfe39a4dac6f769e5c\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6hGhIGiy4GP"
      },
      "source": [
        "# Part 1: Features of text\n",
        "How do we apply machine learning on text? We can't directly use the text as input to our algorithms. We need to convert them to features. In this notebook, we will explore a simple way of converting text to features.\n",
        "\n",
        "Let us download a few documents off Wikipedia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpUmCoEr2R3J"
      },
      "source": [
        "topic1 = 'Giraffe'\n",
        "topic2 = 'Elephant'\n",
        "\n",
        "wikipedia.set_lang('en')\n",
        "\n",
        "eng1 = wikipedia.page(topic1).content\n",
        "eng2 = wikipedia.page(topic2).content\n",
        "\n",
        "wikipedia.set_lang('fr')\n",
        "\n",
        "fr1 = wikipedia.page(topic1).content\n",
        "fr2 = wikipedia.page(topic2).content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj7RlhMiO5kd"
      },
      "source": [
        "This is what the text looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW0G-t912UXZ"
      },
      "source": [
        "fr2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZkmNJ7XO9xX"
      },
      "source": [
        "We need to clean this up a bit. Let us remove all the special characters and keep only 26 letters and space. Note that this will remove accented characters in French also. We are also removing all the numbers and spaces. So this is not an ideal solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5yf5P9pPI4t"
      },
      "source": [
        "def cleanup(text):\n",
        "  text = text.lower()  # make it lowercase\n",
        "  text = re.sub('[^a-z]+', '', text) # only keep characters\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrOjC32fRuTK"
      },
      "source": [
        "eng1 = cleanup(eng1)\n",
        "eng2 = cleanup(eng2)\n",
        "fr1 = cleanup(fr1)\n",
        "fr2 = cleanup(fr2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIdqvL2G-LqL"
      },
      "source": [
        "print(eng1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXFTWwd0rk63"
      },
      "source": [
        "Now let us calculate the frequency of the character n-grams. N-grams are groups of characters of size n. A unigram is a single character and a bigram is a group of two characters and so on.\n",
        "\n",
        "Let us count the frequency of each character in a text and plot it in a histogram."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3Lz3YUjN0L5"
      },
      "source": [
        "# convert a tuple of characters to a string\n",
        "def tuple2string(tup):\n",
        "  st = ''\n",
        "  for ii in tup:\n",
        "    st = st + ii\n",
        "  return st\n",
        "\n",
        "# convert a tuple of tuples to a list of strings\n",
        "def key2string(keys):\n",
        "  return [tuple2string(i) for i in keys]\n",
        "\n",
        "# plot the histogram\n",
        "def plothistogram(ngram):\n",
        "  keys = key2string(ngram.keys())\n",
        "  values = list(ngram.values())\n",
        "\n",
        "  # sort the keys in alphabetic order\n",
        "  combined = zip(keys, values)\n",
        "  zipped_sorted = sorted(combined, key=lambda x: x[0])\n",
        "  keys, values = map(list, zip(*zipped_sorted))\n",
        "  plt.bar(keys, values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHD62zbZcwAB"
      },
      "source": [
        "Let us compare the histograms of English pages and French pages. Can you spot a difference?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKcGRgH6b0KP"
      },
      "source": [
        "unigram_eng1 = Counter(ngrams(eng1,1))\n",
        "plothistogram(unigram_eng1)\n",
        "plt.title('English 1')\n",
        "plt.show()\n",
        "unigram_eng2 = Counter(ngrams(eng2,1))\n",
        "plothistogram(unigram_eng2)\n",
        "plt.title('English 2')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDM_UhCL2QLt"
      },
      "source": [
        "unigram_fr1 = Counter(ngrams(fr1,1))\n",
        "plothistogram(unigram_eng1)\n",
        "plt.title('French 1')\n",
        "plt.show()\n",
        "unigram_fr2 = Counter(ngrams(fr2,1))\n",
        "plothistogram(unigram_fr2)\n",
        "plt.title('French 2')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxgrdZLKdkAB"
      },
      "source": [
        "We can see that the unigrams for French and English are very similar. So this is not a good feature if we want to distinguish between English and French. Let us look at bigrams."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmRCxItx2T9W"
      },
      "source": [
        "bigram_eng1 = Counter(ngrams(eng1,2)) # bigrams\n",
        "plothistogram(bigram_eng1)\n",
        "plt.title('English 1')\n",
        "plt.show()\n",
        "\n",
        "bigram_eng2 = Counter(ngrams(eng2,2))\n",
        "plothistogram(bigram_eng2)\n",
        "plt.title('English 2')\n",
        "plt.show()\n",
        "\n",
        "bigram_fr1 = Counter(ngrams(fr1,2))\n",
        "plothistogram(bigram_eng1)\n",
        "plt.title('French 1')\n",
        "plt.show()\n",
        "\n",
        "bigram_fr2 = Counter(ngrams(fr2,2))\n",
        "plothistogram(bigram_fr2)\n",
        "plt.title('French 2')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-egsHMIg5Rp"
      },
      "source": [
        "Another way to visualize bigrams is to use a 2-dimensional graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EaPJgtaVxZM"
      },
      "source": [
        "def plotbihistogram(ngram):\n",
        "  freq = np.zeros((26,26))\n",
        "  for ii in range(26):\n",
        "    for jj in range(26):\n",
        "      freq[ii,jj] = ngram[(chr(ord('a')+ii), chr(ord('a')+jj))]\n",
        "  plt.imshow(freq, cmap = 'jet')\n",
        "  return freq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7jq3AwnVzQT"
      },
      "source": [
        "bieng1 = plotbihistogram(bigram_eng1)\n",
        "plt.show()\n",
        "bieng2 = plotbihistogram(bigram_eng2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXPTOj67WsPT"
      },
      "source": [
        "bifr1 = plotbihistogram(bigram_fr1)\n",
        "plt.show()\n",
        "bifr2 = plotbihistogram(bigram_fr2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGOEHcyGokD0"
      },
      "source": [
        "Let us look at the top 10 ngrams for each text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dk2TkzTno8vb"
      },
      "source": [
        "from IPython.core.debugger import set_trace\n",
        "\n",
        "def ind2tup(ind):\n",
        "  ind = int(ind)\n",
        "  i = int(ind/26)\n",
        "  j = int(ind%26)\n",
        "  return (chr(ord('a')+i), chr(ord('a')+j))\n",
        "\n",
        "def ShowTopN(bifreq, n=10):\n",
        "  f = bifreq.flatten()\n",
        "  arg = np.argsort(-f)\n",
        "  for ii in range(n):\n",
        "    print(f'{ind2tup(arg[ii])} : {f[arg[ii]]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HeWNh_q0QZ1"
      },
      "source": [
        "print('\\nEnglish 1:')\n",
        "ShowTopN(bieng1)\n",
        "print('\\nEnglish 2:')\n",
        "ShowTopN(bieng2)\n",
        "print('\\nFrench 1:')\n",
        "ShowTopN(bifr1)\n",
        "print('\\nFrench 2:')\n",
        "ShowTopN(bifr2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kDovOP4l98z"
      },
      "source": [
        "We observe that the bigrams are similar across different topics but different across languages. Thus, the bigram frequency is a good feature for distinguishing languages, but not for distinguishing topics.\n",
        "\n",
        "Thus, we were able to convert a many-dimensional input (the text) to 26 dimesions (unigrams) or 26*26 dimensions (bigrams).\n",
        "\n",
        "\n",
        "A few ways to explore:\n",
        "1. Try with different languages.\n",
        "2. The topics we used are quite similar, wikipedia articles of 'elephant' and 'giraffe'. What happens if we use very different topics? What if we use text from another source than Wikipedia?\n",
        "3. How can we use and visualize trigrams and higher n-grams?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZJfjIHk-oHV"
      },
      "source": [
        "# Part 2: Written numbers\n",
        "\n",
        "We will use a subset of the MNIST dataset. Each input character is represented in a 28*28 array. Let us see if we can extract some simple features from these images which can help us distinguish between the digits.\n",
        "\n",
        "Load the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNsLJSr6wGY0"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "#loading the dataset\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVNr144WAUZO"
      },
      "source": [
        "Extract a subset of the data for our experiment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3MN8ddxAASZ"
      },
      "source": [
        "no1 = train_X[train_y==1,:,:]\n",
        "no0 = train_X[train_y==0,:,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePXCs0qyCLpc"
      },
      "source": [
        "Let us visualize a few images here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQeyZSh-Arpc"
      },
      "source": [
        "for ii in range(5):\n",
        "  plt.subplot(1, 5, ii+1)\n",
        "  plt.imshow(no1[ii,:,:])\n",
        "plt.show()\n",
        "for ii in range(5):\n",
        "  plt.subplot(1, 5, ii+1)\n",
        "  plt.imshow(no0[ii,:,:])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g-Tg7EKDz96"
      },
      "source": [
        "suNow, let us start with a simple feature: the sum of all pixels and see how good this feature is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8SztDk7CyZc"
      },
      "source": [
        "sum1 = np.sum(no1>0, (1,2)) # threshold before adding up\n",
        "sum0 = np.sum(no0>0, (1,2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oW3XCOCE7Zv"
      },
      "source": [
        "Let us visualize how good this feature is: (X-axis is mean, y-axis is the digit)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8PIe8o_DPpU"
      },
      "source": [
        "plt.hist(sum1, alpha=0.7);\n",
        "plt.hist(sum0, alpha=0.7);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_hToEepFtl2"
      },
      "source": [
        "We can already see that this feature separates the two classes quite well.\n",
        "\n",
        "Let us look at another, more complicated feature. We will count the number black pixels that are surrounded on four sides by non-black pixels, or \"hole pixels\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwEnlm6RFFej"
      },
      "source": [
        "def cumArray(img):\n",
        "  img2 = img.copy()\n",
        "  for ii in range(1, img2.shape[1]):\n",
        "    img2[ii,:] = img2[ii,:] + img2[ii-1,:]  # for every row, add up all the rows above it.\n",
        "  img2 = img2>0\n",
        "  return img2\n",
        "\n",
        "def getHolePixels(img):\n",
        "  im1 = cumArray(img)\n",
        "  im2 = np.rot90(cumArray(np.rot90(img)), 3) # rotate and cumulate it again for differnt direction\n",
        "  im3 = np.rot90(cumArray(np.rot90(img, 2)), 2)\n",
        "  im4 = np.rot90(cumArray(np.rot90(img, 3)), 1)\n",
        "  hull =  im1 & im2 & im3 & im4 # this will create a binary image with all the holes filled in.\n",
        "  hole = hull & ~ (img>0) # remove the original digit to leave behind the holes\n",
        "  return hole"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw3HjgnupUEI"
      },
      "source": [
        "Visualize a few:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0sjr23NYEFe"
      },
      "source": [
        "imgs = [no1[456,:,:],  no0[456,:,:]]\n",
        "for img in imgs:\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.imshow(getHolePixels(img))\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.imshow(img)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS-4erNXtxMi"
      },
      "source": [
        "Now let us plot the number of hole pixels and see how this feature behaves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dpm1dRgsety8"
      },
      "source": [
        "hole1 = np.array([getHolePixels(i).sum() for i in no1])\n",
        "hole0 = np.array([getHolePixels(i).sum() for i in no0])\n",
        "\n",
        "plt.hist(hole1, alpha=0.7);\n",
        "plt.hist(hole0, alpha=0.7);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UjCBHpJ31yq"
      },
      "source": [
        "This feature works even better to distinguish between one and zero.\n",
        "\n",
        "\n",
        "Now let us try the number of pixels in the 'hull' or the number with the holes filled in:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPtJ8eqolAOf"
      },
      "source": [
        "def getHullPixels(img):\n",
        "  im1 = cumArray(img)\n",
        "  im2 = np.rot90(cumArray(np.rot90(img)), 3) # rotate and cumulate it again for differnt direction\n",
        "  im3 = np.rot90(cumArray(np.rot90(img, 2)), 2)\n",
        "  im4 = np.rot90(cumArray(np.rot90(img, 3)), 1)\n",
        "  hull =  im1 & im2 & im3 & im4 # this will create a binary image with all the holes filled in.\n",
        "  return hull"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3fOgyYjmJ48"
      },
      "source": [
        "imgs = [no1[456,:,:],  no0[456,:,:]]\n",
        "for img in imgs:\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.imshow(getHullPixels(img))\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.imshow(img)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5rHal_HRWnE"
      },
      "source": [
        "Plotting the number of hull pixels versus the digit:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTLzYZLTRQ_p"
      },
      "source": [
        "hull1 = np.array([getHullPixels(i).sum() for i in no1])\n",
        "hull0 = np.array([getHullPixels(i).sum() for i in no0])\n",
        "\n",
        "plt.hist(hull1, alpha=0.7);\n",
        "plt.hist(hull0, alpha=0.7);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSzH26ElXNri"
      },
      "source": [
        "Let us try one more feature, where we look at the number of boundary pixels in each image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-2czBypXMwT"
      },
      "source": [
        "def minus(a, b):\n",
        "  return a & ~ b\n",
        "\n",
        "def getBoundaryPixels(img):\n",
        "  img = img.copy()>0  # binarize the image\n",
        "  rshift = np.roll(img, 1, 1)\n",
        "  lshift = np.roll(img, -1 ,1)\n",
        "  ushift = np.roll(img, -1, 0)\n",
        "  dshift = np.roll(img, 1, 0)\n",
        "  boundary = minus(img, rshift) | minus(img, lshift) | minus(img, ushift) | minus(img, dshift)\n",
        "  return boundary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-V688jFerXh"
      },
      "source": [
        "imgs = [no1[456,:,:],  no0[456,:,:]]\n",
        "for img in imgs:\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.imshow(getBoundaryPixels(img))\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.imshow(img)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSsxsbCNXcNh"
      },
      "source": [
        "bound1 = np.array([getBoundaryPixels(i).sum() for i in no1])\n",
        "bound0= np.array([getBoundaryPixels(i).sum() for i in no0])\n",
        "\n",
        "plt.hist(bound1, alpha=0.7);\n",
        "plt.hist(bound0, alpha=0.7);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuP04Ao_R0Yz"
      },
      "source": [
        "What will happen if we plot two features together?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl7xWg-WRkAy"
      },
      "source": [
        "# Sum and hull\n",
        "plt.scatter(sum0, hull0, alpha=0.1)\n",
        "plt.scatter(sum1, hull1, alpha=0.1)\n",
        "plt.xlabel('Sum')\n",
        "plt.ylabel('Hull')\n",
        "plt.legend(['0','1'])\n",
        "plt.show()\n",
        "\n",
        "# Sum and hole\n",
        "plt.scatter(sum0, hole0, alpha=0.1)\n",
        "plt.scatter(sum1, hole1, alpha=0.1)\n",
        "plt.xlabel('Sum');\n",
        "plt.ylabel('Hole');\n",
        "plt.legend(['0','1'])\n",
        "plt.show()\n",
        "\n",
        "# Hole and boundary\n",
        "plt.scatter(bound0, hole0, alpha=0.1)\n",
        "plt.scatter(bound1, hole1, alpha=0.1)\n",
        "plt.xlabel('Boundary');\n",
        "plt.ylabel('Hole');\n",
        "plt.legend(['0','1'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JYLmKNFSIT-"
      },
      "source": [
        "Now let us try plotting 3 features together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOKEHIXFaWp_"
      },
      "source": [
        "cl1 = ['class 1']*len(sum1)\n",
        "cl0 = ['class 0']*len(sum0)\n",
        "df = pd.DataFrame(list(zip(np.concatenate((hole0, hole0)), np.concatenate((sum1,sum0)),\n",
        "                           np.concatenate((bound1,bound0)), np.concatenate((cl1, cl0)))),\n",
        "               columns =['Hole', 'Sum', 'Boundary', 'Class'])\n",
        "df.head()\n",
        "fig = px.scatter_3d(df, x='Hole', y='Sum', z='Boundary', color='Class', opacity=0.1)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paDGHlFSd5Fu"
      },
      "source": [
        "Feel free to explore the above graph with your mouse.\n",
        "\n",
        "\n",
        "We have seen that we extracted four features from a 28*28 dimensional image.\n",
        "\n",
        "\n",
        "Some questions to explore:\n",
        "1. Which is the best combination of features?\n",
        "2. How would you test or visualize four or more features?\n",
        "3. Can you come up with your own features?\n",
        "4. Will these features work for different classes other than 0 and 1?\n",
        "5. What will happen if we take more that two classes at a time?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. which is the best comnbinationn of features?\n",
        "When you work with more than two classes (i.e., multi-class classification) in Python, the way you approach and model the problem can vary depending on the machine learning framework or library you are using. Python provides several libraries and tools for multi-class classification, including scikit-learn, TensorFlow, PyTorch, and others. Here's a general overview of what happens when you work with multi-class classification in Python:\n",
        "\n",
        "1. *Data Preparation*:\n",
        "   - You'll typically have a dataset where each data point belongs to one of several distinct classes or categories. These classes can be represented as integers (e.g., 0, 1, 2, ...) or as class labels (e.g., \"cat,\" \"dog,\" \"bird,\" ...).\n",
        "\n",
        "2. *Model Selection*:\n",
        "   - You'll choose a machine learning model or algorithm suitable for multi-class classification. Many classifiers inherently support multi-class problems, including decision trees, random forests, support vector machines, k-nearest neighbors, and neural networks.\n",
        "\n",
        "3. *Model Training*:\n",
        "   - You'll split your dataset into training and testing sets (and possibly validation sets), and you'll train the chosen model on the training data. The model will learn to distinguish between the multiple classes during training.\n",
        "\n",
        "4. *Prediction*:\n",
        "   - After training, you can use the trained model to make predictions on new, unseen data. The model will assign one of the multiple classes to each data point based on its learned patterns and features.\n",
        "\n",
        "5. *Evaluation*:\n",
        "   - You'll evaluate the model's performance using metrics appropriate for multi-class classification, such as accuracy, precision, recall, F1-score, confusion matrix, or log-loss (cross-entropy loss).\n",
        "\n",
        "6. *Visualization*:\n",
        "   - Depending on the number of classes and features, you may use various visualization techniques to understand the model's performance and the distribution of data points across classes. Techniques like confusion matrices and class-wise performance metrics can help assess the model's strengths and weaknesses.\n",
        "\n",
        "Here's an example of using scikit-learn for multi-class classification with a decision tree classifier:\n",
        "\n",
        "python\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Iris dataset (a common multi-class classification dataset)\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree classifier and train it\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "\n",
        "\n",
        "In this example, the Iris dataset is used, which contains three classes (setosa, versicolor, and virginica). The decision tree classifier is trained to classify data points into these three classes, and the model's performance is evaluated using accuracy and a classification report."
      ],
      "metadata": {
        "id": "IuwFPnpi6nq9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2.How would you test or visualize four or more features?**\n",
        "\n",
        "When working with datasets that have four or more features, it becomes essential to employ various techniques to test and visualize the data effectively. Here are some common methods for testing and visualizing datasets with multiple features in Python:\n",
        "\n",
        "*Pair Plots (Pairplot):*\n",
        "\n",
        "Pair plots are an excellent way to visualize pairwise relationships between features in a dataset. You can use libraries like Seaborn to create pair plots that display scatterplots for numerical features and histograms for the diagonal entries (univariate distributions).\n",
        "\n",
        "##python code\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.pairplot(df, hue=\"target_variable\")\n",
        "plt.show()\n",
        "\n",
        "*Correlation Matrix:*\n",
        "\n",
        "Calculating and visualizing the correlation matrix can help you understand the relationships between features. You can use a heatmap to display the correlation values.\n",
        "\n",
        "python code\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "correlation_matrix = df.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
        "plt.show()\n",
        "\n",
        "*3D Scatter Plots:*\n",
        "\n",
        "If you have four numerical features, you can create 3D scatter plots to visualize the relationships between three of them while color-coding the points based on the fourth feature.\n",
        "\n",
        "python code\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(df['feature1'], df['feature2'], df['feature3'], c=df['feature4'])\n",
        "plt.show()\n",
        "\n",
        "*Parallel Coordinates:*\n",
        "\n",
        "Parallel coordinates plots are useful for visualizing high-dimensional data. You can use the Pandas parallel_coordinates function for this purpose.\n",
        "\n",
        "python code\n",
        "from pandas.plotting import parallel_coordinates\n",
        "\n",
        "parallel_coordinates(df, 'target_variable')\n",
        "plt.show()\n",
        "Dimensionality Reduction:\n",
        "Techniques like Principal Component Analysis (PCA) or t-Distributed Stochastic Neighbor Embedding (t-SNE) can reduce the dimensionality of your data and allow you to visualize it in lower-dimensional spaces while preserving important relationships.\n",
        "\n",
        "python\n",
        "Copy code\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "reduced_features = pca.fit_transform(df)\n",
        "plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=df['target_variable'])\n",
        "plt.show()\n",
        "Interactive Visualization:\n",
        "Consider using libraries like Plotly or Bokeh for interactive data exploration, which allows you to create interactive visualizations like scatter plots, histograms, and more with tooltips and zooming capabilities.\n",
        "\n",
        "python code\n",
        "import plotly.express as px\n",
        "\n",
        "fig = px.scatter_3d(df, x='feature1', y='feature2', z='feature3', color='feature4')\n",
        "fig.show()\n",
        "\n",
        "*Statistical Tests:*\n",
        "\n",
        "To test the relationships between features, you can perform statistical tests like ANOVA, Chi-squared test, or regression analysis depending on your data types (numerical or categorical).\n",
        "\n",
        "These methods provide different ways to explore and visualize datasets with four or more features, helping you gain insights into the data's structure, relationships, and potential patterns. The choice of visualization technique will depend on the nature of your data and your specific analysis goals."
      ],
      "metadata": {
        "id": "z57lofnQ68Ix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.Can you come up with your own features?\n",
        "Yes, you can certainly create your own features in Python when working with data or developing applications. Creating custom features often involves manipulating existing data or adding new attributes to your objects or datasets to better suit your needs. Here are some examples of how you can create your own features in Python:\n",
        "\n",
        "*Derived Features:*\n",
        "\n",
        "You can create new features by performing operations on existing features. For example, if you have a dataset with a \"price\" and \"quantity\" feature, you can create a new feature \"total_cost\" by multiplying these two features.\n",
        "\n",
        "python code\n",
        "df['total_cost'] = df['price'] * df['quantity']\n",
        "*Text Features:*\n",
        "If you're working with text data, you can create features like word counts, character counts, or sentiment scores from the text content.\n",
        "\n",
        "python code\n",
        "df['word_count'] = df['text_column'].apply(lambda x: len(x.split()))\n",
        "\n",
        "*Date Features:*\n",
        "\n",
        "When dealing with date-time data, you can create features like day of the week, month, or year from a date column.\n",
        "\n",
        "python code\n",
        "df['day_of_week'] = df['date_column'].dt.dayofweek\n",
        "\n",
        "*Categorical Features:*\n",
        "\n",
        "You can convert categorical features into numerical representations using techniques like one-hot encoding or label encoding.\n",
        "\n",
        "python code\n",
        "df = pd.get_dummies(df, columns=['category'])\n",
        "\n",
        "*Aggregated Features:*\n",
        "\n",
        "You can create features by aggregating data. For example, calculate the mean, sum, or max of a numerical feature for a group of data points.\n",
        "\n",
        "python code\n",
        "mean_sales_by_category = df.groupby('category')['sales'].mean()\n",
        "df['mean_sales_by_category'] = df['category'].map(mean_sales_by_category)\n",
        "\n",
        "*Custom Functions:*\n",
        "\n",
        "You can define custom functions to create features based on complex logic. For instance, you can create a feature that checks if a text column contains certain keywords.\n",
        "\n",
        "python code\n",
        "def contains_keyword(text):\n",
        "    keywords = ['important', 'urgent']\n",
        "    return any(keyword in text for keyword in keywords)\n",
        "\n",
        "df['contains_keyword'] = df['text_column'].apply(contains_keyword)\n",
        "\n",
        "*Feature Engineering:*\n",
        "\n",
        "In machine learning, feature engineering involves creating new features to improve model performance. This can include polynomial features, interaction terms, or transformation of existing features.\n",
        "\n",
        "python code\n",
        "from  preprocessing import PolynomialFeatures\n",
        "\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "*Geospatial Features:*\n",
        "\n",
        "If you're working with geospatial data, you can create features like distances between points, cluster labels, or regions based on coordinates.\n",
        "\n",
        "python code\n",
        "from geopy.distance import great_circle\n",
        "\n",
        "df['distance_to_city_center'] = df.apply(lambda row: great_circle((row['latitude'], row['longitude']), city_center).miles, axis=1)\n",
        "Remember that the creation of custom features should be driven by your understanding of the data and the problem you're trying to solve. Feature engineering is often an iterative process, and it's essential to validate and test the impact of new features on your analysis or machine learning models."
      ],
      "metadata": {
        "id": "CgH4kmrk7BMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#*4.Will these features work for different classes other than 0 and 1?*\n",
        "\n",
        "The features I mentioned in my previous response can certainly be used for different classes other than just binary classes 0 and 1. In fact, these feature creation techniques are generally applicable to a wide range of classification and regression problems where the target variable can take on multiple classes or continuous values.\n",
        "\n",
        "#Here's how these features can work for different classes:\n",
        "\n",
        "*Derived Features:*\n",
        "\n",
        "Features created by performing operations on existing features are not limited to binary classes. You can create derived features that capture relationships between variables for multi-class or regression problems.\n",
        "\n",
        "*Text Features:*\n",
        "\n",
        "Count-based features like word counts or sentiment scores can be useful for sentiment analysis, topic modeling, or any text-based classification problem, regardless of the number of classes.\n",
        "\n",
        "*Date Features:*\n",
        "\n",
        "Date-related features like day of the week, month, or year can be relevant for various classification tasks or regression problems, such as predicting sales for multiple products or forecasting demand over time.\n",
        "\n",
        "*Categorical Features:*\n",
        "\n",
        "One-hot encoding or label encoding can be applied to categorical features with multiple classes, allowing you to represent and work with categorical data effectively.\n",
        "\n",
        "*Aggregated Features:*\n",
        "\n",
        "Aggregated features can summarize data for different categories or groups, making them suitable for multi-class problems where you want to understand patterns within each class.\n",
        "\n",
        "*Custom Functions:*\n",
        "\n",
        "Custom functions for feature creation can be adapted to handle multi-class scenarios by considering class-specific logic or patterns within the data.\n",
        "\n",
        "*Feature Engineering:*\n",
        "Techniques like polynomial features, interaction terms, or feature transformations can be applied to multi-class or regression problems to capture complex relationships between variables.\n",
        "\n",
        "*Geospatial Features:*\n",
        "\n",
        "Geospatial features, such as distances between points or cluster labels, can be relevant for location-based classification tasks, such as categorizing points of interest into multiple classes.\n",
        "\n",
        "The features and feature engineering techniques mentioned earlier are versatile and can be used in a wide range of machine learning and data analysis tasks, including those with multiple classes or regression targets. The key is to adapt these techniques to the specific requirements and characteristics of your problem and dataset."
      ],
      "metadata": {
        "id": "tt7d8UMq7G1F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#*5.What will happen if we take more that two classes at a time?*\n",
        "When you work with more than two classes (i.e., multi-class classification) in Python, the way you approach and model the problem can vary depending on the machine learning framework or library you are using. Python provides several libraries and tools for multi-class classification, including scikit-learn, TensorFlow, PyTorch, and others. Here's a general overview of what happens when you work with multi-class classification in Python:\n",
        "\n",
        "1. *Data Preparation*:\n",
        "\n",
        "    You'll typically have a dataset where each data point belongs to one of several distinct classes or categories. These classes can be represented as integers (e.g., 0, 1, 2, ...) or as class labels (e.g., \"cat,\" \"dog,\" \"bird,\" ...).\n",
        "\n",
        "2. *Model Selection*:\n",
        "\n",
        "   You'll choose a machine learning model or algorithm suitable for multi-class classification. Many classifiers inherently support multi-class problems, including decision trees, random forests, support vector machines, k-nearest neighbors, and neural networks.\n",
        "\n",
        "3. *Model Training*:\n",
        "    You'll split your dataset into training and testing sets (and possibly validation sets), and you'll train the chosen model on the training data. The model will learn to distinguish between the multiple classes during training.\n",
        "\n",
        "4. *Prediction*:\n",
        "    After training, you can use the trained model to make predictions on new, unseen data. The model will assign one of the multiple classes to each data point based on its learned patterns and features.\n",
        "\n",
        "5. *Evaluation*:\n",
        "    You'll evaluate the model's performance using metrics appropriate for multi-class classification, such as accuracy, precision, recall, F1-score, confusion matrix, or log-loss (cross-entropy loss).\n",
        "\n",
        "6. *Visualization*:\n",
        "    Depending on the number of classes and features, you may use various visualization techniques to understand the model's performance and the distribution of data points across classes. Techniques like confusion matrices and class-wise performance metrics can help assess the model's strengths and weaknesses.\n",
        "\n",
        "Here's an example of using scikit-learn for multi-class classification with a decision tree classifier:\n",
        "\n",
        "\n",
        "***from sklearn.datasets import load_iris\n",
        "   from sklearn.model_selection import train_test_split\n",
        "   from sklearn.tree import DecisionTreeClassifier\n",
        "   from sklearn.metrics import accuracy_score, classification_report***\n",
        "\n",
        "# Load the Iris dataset (a common multi-class classification dataset)\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree classifier and train it\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "```\n",
        "\n",
        "In this example, the Iris dataset is used, which contains three classes (setosa, versicolor, and virginica). The decision tree classifier is trained to classify data points into these three classes, and the model's performance is evaluated using accuracy and a classification report."
      ],
      "metadata": {
        "id": "xD5-4jd57K6-"
      }
    }
  ]
}